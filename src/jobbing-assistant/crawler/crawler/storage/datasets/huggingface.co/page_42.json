{"url": "https://huggingface.co/docs/safetensors", "title": "Safetensors", "text": "       Hugging Face        Models  Datasets  Spaces  Posts  Docs     Solutions   Pricing        Log In  Sign Up     Safetensors documentation  Safetensors     Safetensors   \ud83c\udfe1 View all docs AWS Trainium & Inferentia Accelerate Amazon SageMaker Argilla AutoTrain Bitsandbytes Chat UI Competitions Dataset viewer Datasets Diffusers Distilabel Evaluate Google TPUs Gradio Hub Hub Python Library Huggingface.js Inference API (serverless) Inference Endpoints (dedicated) Leaderboards Optimum PEFT Safetensors Sentence Transformers TRL Tasks Text Embeddings Inference Text Generation Inference Tokenizers Transformers Transformers.js timm   Search documentation    main v0.3.2 v0.2.9  EN          Getting started    \ud83e\udd17 Safetensors  Speed Comparison  Tensor Sharing in Pytorch  Metadata Parsing  Convert weights to safetensors    API    Torch API  Tensorflow API  PaddlePaddle API  Flax API  Numpy API    You are viewing main version, which requires installation from source . If you'd like\n\t\t\tregular pip install, checkout the latest stable version ( v0.3.2 ).   Join the Hugging Face community  and get access to the augmented documentation experience   Collaborate on models, datasets and Spaces   Faster examples with accelerated inference   Switch between documentation themes  Sign Up  to get started                    Safetensors  Safetensors is a new simple format for storing tensors safely (as opposed to pickle) and that is still fast (zero-copy). Safetensors is really fast \ud83d\ude80 .   Installation  with pip:   Copied  pip install safetensors  with conda:   Copied  conda install -c huggingface safetensors   Usage   Load tensors   Copied  from safetensors import safe_open\n\ntensors = {} with safe_open( \"model.safetensors\" , framework= \"pt\" , device= 0 ) as f: for k in f.keys():\n        tensors[k] = f.get_tensor(k)  Loading only part of the tensors (interesting when running on multiple GPU)   Copied  from safetensors import safe_open\n\ntensors = {} with safe_open( \"model.safetensors\" , framework= \"pt\" , device= 0 ) as f:\n    tensor_slice = f.get_slice( \"embedding\" )\n    vocab_size, hidden_dim = tensor_slice.get_shape()\n    tensor = tensor_slice[:, :hidden_dim]   Save tensors   Copied  import torch from safetensors.torch import save_file\n\ntensors = { \"embedding\" : torch.zeros(( 2 , 2 )), \"attention\" : torch.zeros(( 2 , 3 ))\n}\nsave_file(tensors, \"model.safetensors\" )   Format  Let\u2019s say you have safetensors file named model.safetensors , then model.safetensors will have the following internal format:    Featured Projects  Safetensors is being used widely at leading AI enterprises, such as Hugging Face , EleutherAI ,\u00a0and StabilityAI . Here is a non-exhaustive list of projects that are using safetensors:  huggingface/transformers  ml-explore/mlx  huggingface/candle  AUTOMATIC1111/stable-diffusion-webui  Llama-cpp  microsoft/TaskMatrix  hpcaitech/ColossalAI  huggingface/pytorch-image-models  CivitAI  huggingface/diffusers  coreylowman/dfdx  invoke-ai/InvokeAI  oobabooga/text-generation-webui  Sanster/lama-cleaner  PaddlePaddle/PaddleNLP  AIGC-Audio/AudioGPT  brycedrennan/imaginAIry  comfyanonymous/ComfyUI  LianjiaTech/BELLE  alvarobartt/safejax  MaartenGr/BERTopic  <  >  Update on GitHub      Speed Comparison \u2192    Safetensors  Installation  Usage  Load tensors  Save tensors  Format  Featured Projects        "}